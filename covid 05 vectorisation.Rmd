---
title: "40 ans de PMP"
author: "cb et mb"
date: "30/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,include=TRUE,cache=TRUE,warning=FALSE,message = FALSE )
library(readr) #lire les donnees
library(tidyverse) #on ne peut plus s'en passer
library(quanteda) # les bases du nlp
library(spacyr)
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("kbenoit/quanteda.dictionaries")
library(stringr) #manipuler le texte
library(pals)
library(ape) #pour de jolis dendogrammes
library(tidytext) #du nlp
library(proustr) #un package sympa pour les dicos
library(mixr)
library(widyr)
library(reshape2) 
library(igraph) #classic des reseaux
library(ggrepel) #gestion des labels
library(ggraph) 
library(gglorenz) #courbe de concentration
library('extrafont') #c'est pour igraph
# Import des fontes du système - cela peut être long
#font_import()
#fonts() # Liste des polices disponibles
#loadfonts(device = "pdf") # Indiquer device = "pdf" pour produire un pdf 
library(stringi)
library(gridExtra)
library(cleanNLP) 
library(text2vec)
library(wordVectors)
library(tsne)
library(ape)
library(lubridate)
```

# un peu de vectorisation

Nous avons traité de cooccurences au niveau des textes, mais ce qu'apporte les mots en terme sémantique dépend une distance.
 prenons la phrase : " le mille feuille administratif empêche l'innovation" 
 
Les cooccurence peuvent être nuancées en prenant en compte des sac de mots (bag of words), les coocuurence peuvent être nuancé par la distance des mots.

On peu se donner une représentation des mots dans un espace arbitraire ( et de grande dimension) dans le but de bien représenter ces coocurrences, sur la base du principe que si souvent deux mots sont employés ensembles, c'est qu'il se décrivent l'un l'autre. La redondance est une synonymie masquée, Dans ce formalisme la coorélation est le cosinus de l'angle formé par ces deux vecteurs dans l'espace abstrait de grande dimension qu'on cherche à construire sous la condition de bien rendre compte des coocuurences.

## preparation du texte d'entrée

On sélectionne les POS pertienents, puis à l'aire d'une boucle on crée pour chaque document, un champs texte avec la succesion des lemmes retenus. Cette opération revient à condenser le texte en éliminant les marqueurs grammaticaux, on ne garde que ce qui est intrinsèquement signifiant.

Des opérations accéssoire sont réalisée dans la boucle comme la tranlittération ( var word2vec accepte marque les caractère français, notamment les accents) et la suppression de la ponctuation.


```{r vector2}

Vocab<-readRDS(file = "objCovid.rds")

updated_vocab <-Vocab$token %>% filter( upos %in% c('NOUN'))

# on reconstitution les textes
# détail détechnique on traite les accents avec iconv 


foo1<-data.frame(matrix( nrow=1, ncol=2))
foo1$id <-0
foo1$text<-"xxx"
foo1<- foo1 %>% as.data.frame() %>% select(-X1,-X2)
foo<-foo1
for (i in 1:526662) {
updated_vocabi<-updated_vocab %>% filter(doc_id==i)
foo$id<-i
foo$text <- paste(updated_vocabi["lemma"], sep= " ") #on retient les lemmes
foo$text<-substring(foo$text, 3) #on elimine les 3 premier caractères
foo$text<-gsub("-", "", foo$text, fixed=TRUE) #on supprime les slash
foo$text<-gsub("[[:punct:]]", "", foo$text)
foo$text <- iconv(foo$text, to="ASCII//TRANSLIT//IGNORE")
foo$text<-gsub("NA", "", foo$text)
foo$text<-gsub(".*xxx.*", "", foo$text)
foo1<-rbind(foo1,foo)
}
foo1<-foo1 %>% filter(id>0) %>%select(text)
saveRDS(foo1, "vocabulaire.rds")

#construction du vocabulaire
vocabulaire<-foo1  %>%
  unnest_tokens(word, text)
vocabulaire$line<-row.names(vocabulaire)
vocabulaire$abstract<-gsub("\\..*$", "", vocabulaire$line) #à partir d'un point, tous les caractères, autant de fois , jusqu'à la fin

voca_ag <-vocabulaire %>%group_by(word)%>% mutate(n=1)%>% summarise(n=sum(n))

#préparation pour Word2vec
write.table(foo1, file="text_tok.txt", sep=",",row.names=FALSE,col.names = FALSE)
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="text_tok.txt",destination="text_tok2.txt",lowercase=T,bundle_ngrams=3)
```


## Apprentissage du modèle

Le modèle vectoriel du vocabulaire du corpus dépend de 3 paramètres :

 * le nombre de vecteurs qui assure la précision , 
 * la taille des bag of words qui favorise les mots proches ou plus éloignés, 
 * la fréquence minimales des termes employés.

Dans cette version, on choisit un nombre de vecteurs modéré 200, une fenêtre de 5 termes, et une fréquence minimale de 10 mots.12767 mots servent de vocabulaire, 1,97 millions de mots sont employés pour estimer / entraîner le modèle. Une grande partie ( environ 41000 - 2600) est constitué de termes uniques qui ne peuvent être modélisés.

```{r vector3}

#Création et entraînement du modèle vectoriel
model = train_word2vec("text_tok2.txt","vec.bin",vectors=200,threads=4,window=5,iter=1000,
                         negative_samples=0,force=TRUE, min_count=10)

```
## Des résultats
Le modèle est en fait un tableau des éléments du vocabulaire et des 100 vecteurs qui les représentent. On peut y rechercher les vecteurs qui lui sont le plus proches. ON notera les bigramm, par exemple "loi_decentralisation".


```{r vector3a,fig.height=9}

foo<-model %>% closest_to(~"masque",50)
#head(foo,25)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="blue4",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches du vecteur gestion")
g1
```
l'intérétet de la fonction est que l'on peut rechercher des vecteurs plus abstraits, appelons les  concepts, qui sont le résultat de vecteurs combinés. Management et gestion étant a priori la même chose, avec sans doute une saveur particulière, la somme des deux vecteurs et donc le vecteurs résultant, peut représenter une bonne approche du concept de la gestion.

Dans notre cas, ce concept est un art associé à la compta et à la gestion des humains. C'est un peu plus un art qu'un outil, l'objet public le structure par la loi de décentralisation et l'administration, il est souvent territorial.  

```{r vector3b,fig.height=9}

foo<-model %>% closest_to(~"masque"+"gel",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g2<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches du concept de la gestion : gestion+management")
g2
```

Essayons la soustraction. Celle-ci revient à identifier les termes liés au premier vecteur, et à ceux qui sont inversement lié à l'élément qu'on soustrait : le "management" si lui ote les aspects spécifiques à la gestion, se présente comme un regard, une manière de considérer, plutôt scientifique mais aussi comme un art pourlequel se pose une question de champ et de légitimité. 

```{r vector3c,fig.height=9}

foo<-model %>% closest_to(~"test",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g2<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches du concept de la gestion : management-gestion")
g2
```
Atttention la soustraction n'est pas commutative. L'opération syméttrique gestion- management, a pour but d'identifier ce qui est liau vecteurs gestion mais qui lui est spécifiques. Les aires urbaines et les collectivités lui sont fortement associés, mais ce qui en fait la spécificité c'est le contrôle et son rôle de support. Support des politiques publiques?

```{r vector3d,fig.height=9}

foo<-model %>% closest_to(~"masque"+"test",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g2<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches du concept de la gestion : gestion-management")
g2
```

On 
```{r vector3e,fig.height=9}

tastes = model[[c("masque","test"),average=F]]
c1c2 = model %>% cosineSimilarity(tastes)
# Filter to the top 20 sweet or salty.
c1c2 = c1c2[
  rank(-c1c2[,1])<40 |
  rank(-c1c2[,2])<40,
  ]

c1c2<-c1c2 %>% as.data.frame() 
c1c2$label <-row.names(c1c2)
ggplot(c1c2, aes(x=masque,y=test,label=label))+geom_point(size=2,color="chartreuse3")+
  geom_text_repel(aes(label = label)) +
  theme_minimal()+  xlim(-.1,0.5)+ylim(-0.1,0.5)
```

```{r vector3f,fig.height=9}


q_words = c("gel", "masque", "distance","gel","confinement")
term_set = lapply(q_words, 
                  function(q_word) {
                    nearest_words = model %>% closest_to(model[[q_word]],40)
                    nearest_words$word
                  }) %>% unlist
subset = model[[term_set,average=F]]

library(Rtsne)
library(RColorBrewer)
# run Rtsne with default parameters
set.seed(57)
rtsne_out <- Rtsne(as.matrix(subset), perplexity=15)
# plot the output of Rtsne into d:\\barneshutplot.jpg file of 2400x1800 dimension
#jpeg("fig.jpg", width=2400, height=1800)
color.vec = c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58", "seagreen1", "seagreen4", "slateblue4")
terms<-as.data.frame(rownames(subset))
terms$word<-terms[,1] 
#terms<-terms %>% left_join(clus, by = "word")

plot(rtsne_out$Y, t='n')
#count(terms, clus)$n[2]
text(rtsne_out$Y, labels=rownames(subset),cex=0.9)#col=color.vec[terms$clus])

#embedding <- as.data.frame(model)
```

## une typologie en guise d'analyse factorielle


```{r vector4, fig.height=15,fig.width=12}

#Un premier clustering
set.seed(10)
centers = 16
clustering = kmeans(model,centers=centers,iter.max = 40) 
sapply(sample(1:centers,centers),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:15])
})
# un mmodèle hierachique
library(apcluster)
d <- cor(t(model))   # find distance matrix 
c<-dist(d)
hc <- hclust(c,method = "ward.D2")                # apply hierarchical clustering 
clus = cutree(hc, 10)

tol18rainbow=c("#771155", "#AA4488", "#CC99BB", "#114477", "#4477AA", "#77AADD", "#117777", "#44AAAA", "#77CCCC", "#777711", "#AAAA44", "#DDDD77", "#774411", "#AA7744", "#DDAA77", "#771122", "#AA4455", "#DD7788")
# I assume here, the word before the "_" tells us how to colour the label
# check the TYPE numbers are correct

hh<-as.phylo(hc)
#plot(hh, type = "fan", cex = 0.5,edge.color = "grey", tip.color = tol18rainbow[clus], label.offset = 0.2)
## Creating the vector of labels
my_labels <- hh$tip.label

## "Removing" the unwanted labels (e.g. label 1, 2 and 7)

p=0.75
for (i in 1:2604) {
  r=runif(1)
if (r>p){
my_labels[i] <- ""
}
}
my_labels
plot(hh, type = "fan", cex = 0.5,edge.color = "grey",show.tip.label = FALSE)

## Adding the labels
tiplabels(my_labels, cex = 0.5,frame = "none")
```
## rtsne
pour obtenir une représentation spatiale du vocabulaire une bonne approche est celle proposée par tsne

l'enjeu est de controler le paramètre de perplexité.  Ceoui-ci tend à "gondoler l'espace ", plus il est élevé et plus les objets situés dans des zones à grande dentsités sont étalés. 

un jeu d'essai erreur conduit à recommander une valeur de 
représentation spatiale

```{r vector5, fig.height=12,fig.width=12}


library(ggrepel)
tsne<-tsne(model,k=2,perplexity=15, epoch=100)



words<-as.data.frame(attributes(clustering[[1]]))
cluster<-as.data.frame(clustering[[1]])
cluster$cluster<-as.factor(cluster[,1])


tsne2<-as.data.frame(tsne)
foo<-cbind(tsne2,clus)
foo$label<-as.factor(row.names(foo))
foo$clus<-as.factor(clus)

# 4000 signifie 1 pour 1000 mots
colors<-c("yellowgreen","steelblue4","springgreen3","tan2","salmon","firebrick","darkblue","chartreuse3","aquamarine4","coral3","darkorchid3","purple","sienna", "purple")
ggplot(foo,aes(x=V1,y=V2,group=clus))+theme_minimal()+
  geom_text(aes(label = label,color=clus),cex=3)+scale_color_manual(values=colors)




```





# Topic analysis

L'analyse de topic est devenu un classique tu traitement du langage naturel. proposé par  blei en 2003, elle a connu différente variante dont une nous serait utile 
mais la question du temps


il faut lemmatiser
du lda avec stm

c'est un modèle particulier de topic qu permet d'introduire des co-variables. La nôtre est l'année.
Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G. Rand. "Structural Topic Models for Open-Ended Survey Responses." American Journal of Political Science 58, no 4 (2014): 1064-1082.

## préparation des données

On prépare les données
utilisation de la collocation pour les bigrams qui correspondent à des expressions. Puis on converti dans le format stm requis

```{r stm01}
text_filtered<-readRDS("vocabulaire.rds")

df_userc<-df%>%select(id,media,urls,month,tweet_typ)
df_userc<-cbind(df_userc,text_filtered)
df_userc$text<-gsub(".*xxx.*", "", df_userc$text)

dfm_sample<-sample_n(df_userc,50000)

corp<-corpus(dfm_sample$text, docvars=(dfm_sample))# corps des auteueut

set.seed(100)
library("stm")

#head(cols <- textstat_collocations(corp, size = 2, min_count = 2), 10)

dfm<-dfm(corp, tolower = TRUE,remove_punct = TRUE, remove_numbers = FALSE,remove = stopwords("french"),
  stem = FALSE,  verbose = quanteda_options("verbose"))
dfm_stm <- convert(dfm, to = "stm")
```

## recherche du nombre de sujets

on teste différentes solutions pour K , on dispose de 4 indicateurs

 * La cohérence sémantique est une mesure liée à l'information mutuelle ponctuelle qui a été introduite
dans un article de David Mimno, Hanna Wallach et collègues (voir références), :l'idée centrale est que dans les modèles sémantiquement cohérents, les mots qui sont les plus probables sous un sujet devraient se retrouver dans le même document.
 * l'exclusivité
 * le résidu : entre la prediction et l'effectif empirique
 * held hout likehood ; c'est la vraissemblance claculée sur un autre jeu que celui qui a servi à l'estimer.
 la meilleure et la plus courte est à 24 solutions

```{r stm02}

#kresult <- searchK(dfm_stm$documents, dfm_stm$vocab, K = c(20,22,24,26,28,30,32), prevalence =~  s(month), data = dfm_stm$meta)
#plot(kresult)
```

Il semble qu'une solution à 26 topics semble intéressante

## calcul du model

la prévalence : l'effet du temps que nous avons déjà dénoté

on présente les résultats en par des nuages de point propre à chacun des topics où la taille des mots est proportionnel à deux indicateurs
 *  la probabilité que le mot appartiennent au topic, mais un mot fréquent aura une forte proba dans tout les topics
 * flex qui identifie les mots qui distinguent le topics
 

```{r stm03}
k=26
model.stm <- stm(dfm_stm$documents, 
                 dfm_stm$vocab, 
                 K = k, 
                 data = dfm_stm$meta, 
                 init.type = "Spectral", 
                 prevalence =~ s(month),
                 interactions = TRUE,
                 verbose = FALSE) # this is the actual stm call
label<-as.data.frame(labelTopics(model.stm, n = 28)$Lift)
plot(model.stm, type = "summary", labeltype="frex",text.cex = 0.8,n=5)

```


la description

```{r stm04, fig.width=12}

par(mfrow = c(4,7) , mar = c(0,0,0,0))
for (i in seq_along((1:k)))
{
  cloud(model.stm, topic = i, type ="model", max.words = 50, colors="darkblue", random.order=FALSE)
  text(x=0.5, y=1, paste0("topic",i))

}


par(mfrow = c(4,7) , mar = c(0,0,0,0))
for (i in seq_along((1:k)))
{
cloud(model.stm, topic = i,type = c("model", "documents"), dfm,thresh = 0.9, max.words = 50, colors="firebrick")
   text(x=0.5, y=1, paste0("topic",i))

}
par(mfrow = c(4,7) , mar = c(0,0,0,0))


model.stm.labels <- labelTopics(model.stm, 1:k)

dfm_stm$meta$datum <- as.numeric(dfm_stm$meta$month)

model.stm.ee <- estimateEffect(1:k ~ s(Year), model.stm, meta = dfm_stm$meta)
# Now we plot this estimation for a handful of topics (here 9 randomly chosen ones).
par(mfrow = c(7,4) , mar = c(1,0,2,0))
for (i in seq_along((1:k)))
{
  plot(model.stm.ee, "Year", method = "continuous", topics = i, main = paste0(model.stm.labels$frex[i,1:2], collapse = "-"), printlegend = T)

}
```


retrouver les textes liés aux topic

et regarder les liens ( plutôt positif) entre les topics. L'absence de lien dénote l'existance possible d'une relation négative

```{r stm06, fig.width=9}
b<-NULL
for (i in seq_along((1:k)))
{

  a<-paste0(model.stm.labels$frex[i,1:3], collapse = "-")
b<-rbind(b,a)
}
b
label<-as.data.frame(b)
thoughts3 <- findThoughts(model.stm, texts = dn$text, n = 3, topics = 1)$docs[[1]]
thoughts3
thoughts19 <- findThoughts(model.stm, texts = dn$text, n = 2, topics = 19)$docs[[1]]
thoughts19

topicor<-topicCorr(model.stm, method = "simple", cutoff=0,verbose = TRUE)
topicor$poscor
plot.topicCorr(topicor, vertex.color = "chartreuse3", vlabel=b, vertex.label.cex = .8, vertex.label.color = "black", vertex.size =10,edge.size=2)

```


https://juliasilge.com/blog/sherlock-holmes-stm/
You can also embed plots, for example:

ici un modèle spécifique :
https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf



les topics des textes :


l'entropie maximale est de 39,07 On mesure la différence entre l'entropie observée et l'entropie minimale comme mesure du gain de diversité

```{r word6, fig.width=12}
k=28
doc_topic <-as.data.frame(model.stm$theta)
doc_topic <- cbind(dn,doc_topic)%>%select(-Abstract, -text,-Key,-Title,-decade,-nchar,-n_words)%>% mutate(Year=as.factor(Year))

doc_topic1<-melt(doc_topic)

topic<-doc_topic1 %>% 
   group_by(Year,variable) %>% 
   summarise(top=mean(value))%>%ungroup()  %>% mutate(Year=as.numeric(Year)) 

 ggplot(topic,aes(x=Year,y=top)) +
   geom_line(stat="identity",color="Orange3")+
   geom_smooth()+theme_minimal()+facet_wrap(vars(variable),ncol=4, scales ="free")
```

```{r entropie}

 trop<-topic%>% mutate(pLp = top*log(top)) %>% group_by(Year)  %>%summarise(entropie=-sum(pLp)) %>% ungroup 
 
  ggplot(trop,aes(x=Year,y=entropie)) +
   geom_line(stat="identity",color="darkgreen",size=1.5)+
   geom_smooth()+theme_minimal() +
  labs(title="Evolution de l'entropie",y="entropie/diversité",x="Année de publication",caption="corpus PMP")

```

# Références

